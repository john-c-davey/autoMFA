@article{VaR-MFA,
author = {Kwangyee Ko and Jangsun Baek},
title = {{VaR} {E}stimation {U}sing {S}kewed {M}ixture {M}odels and {V}arious {M}ixtures of {F}actor {A}nalyzers},
journal = {Journal of the {K}orean {D}ata and {I}nformation {S}cience {S}ociety},
volume = {29},
number = {3},
pages = {769 -- 778},
year = {2018},
URL = {http://www.kdiss.org/journal/view.html?spage=769&volume=29&number=3#body01},
}


@article{10.1214/17-AOAS1049,
author = {Antonello Maruotti and Jan Bulla and Francesco Lagona and Marco Picone and Francesca Martella},
title = {{D}ynamic {M}ixtures of {F}actor {A}nalyzers to {C}haracterize {M}ultivariate {A}ir {P}ollutant {E}xposures},
volume = {11},
journal = {The {A}nnals of {A}pplied {S}tatistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1617 -- 1648},
keywords = {AECM algorithm, dimensionality reduction, Hidden Markov models, three-step algorithm},
year = {2017},
doi = {10.1214/17-AOAS1049},
}


@InProceedings{LeeCyto,
author="Lee, Sharon X.",
title="CytoFA: {A}utomated {G}ating of {M}ass {C}ytometry {D}ata via {R}obust {S}kew {F}actor {A}nalzyers",
booktitle="{A}dvances in {K}nowledge {D}iscovery and {D}ata {M}ining",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="514--525",
abstract="Cytometry plays an important role in clinical diagnosis and monitoring of lymphomas, leukaemia, and AIDS. However, analysis of modern-day cytometric data is challenging. Besides its high-throughput nature and high dimensionality, these data typically exhibit complex characteristics such as multimodality, asymmetry, heavy-tailness and other non-normal characteristics. This paper presents cytoFA, a novel data mining approach capable of clustering and performing dimensionality reduction of high-dimensional cytometry data. Our approach is also robust against non-normal features including heterogeneity, skewness, and outliers (dead cells) that are typical in flow and mass cytometry data. Based on a statistical approach with well-studied properties, cytoFA adopts a mixtures of factor analyzers (MFA) to learn latent nonlinear low-dimensional representations of the data and to provide an automatic segmentation of the data into its comprising cell populations. We also introduce a double trimming approach to help identify atypical observations and to reduce computation time. The effectiveness of our approach is demonstrated on two large mass cytometry data, outperforming existing benchmark algorithms. We note that while the approach is motivated by cytometric data analysis, it is applicable and useful for modelling data from other fields.",
isbn="978-3-030-16148-4",
doi = {10.1007/978-3-030-16148-4_40},
}

@INPROCEEDINGS{facedetection,

  author={Yang, {Ming-Hsuan} and Ahuja, Narendra and Kriegman, David},

  booktitle={Proceedings 1999 {I}nternational {C}onference on {I}mage {P}rocessing ({C}at. 99CH36348)}, 

  title={Face {D}etection using a {M}ixture of {F}actor {A}nalyzers}, 

  year={1999},

  volume={3},

  number={},

  pages={612-616 vol.3},

  doi={10.1109/ICIP.1999.817188}}


@inproceedings{m2000,

  author    = {McLachlan, G.J. and Peel, D.},
  title     = {Mixtures of {F}actor {A}nalyzers},
  booktitle = {Proceedings of
the {S}eventeenth {I}nternational {C}onference on {M}achine {L}earning},
  series    = {Langley, {P} ({E}d.)},
  year      = 2000,
  pages     = {599--606},
  publisher = {Morgan {K}aufmann},
  address   = {San {F}rancisco},
}

@article{ECM,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337198},
 abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler CM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
 author = {{Xiao-{L}i} {M}eng and {{D}onald {B}.} {R}ubin},
 journal = {Biometrika},
 number = {2},
 pages = {267--278},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Maximum {L}ikelihood {E}stimation via the {ECM} {A}lgorithm: {A} {G}eneral {F}ramework},
 volume = {80},
 year = {1993}
}

@article{AECM,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346009},
 abstract = {Celebrating the 20th anniversary of the presentation of the paper by Dempster, Laird and Rubin which popularized the EM algorithm, we investigate, after a brief historical account, strategies that aim to make the EM algorithm converge faster while maintaining its simplicity and stability (e.g. automatic monotone convergence in likelihood). First we introduce the idea of a `working parameter' to facilitate the search for efficient data augmentation schemes and thus fast EM implementations. Second, summarizing various recent extensions of the EM algorithm, we formulate a general alternating expectation-conditional maximization algorithm AECM that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations. We illustrate these methods using multivariate t-models with known or unknown degrees of freedom and Poisson models for image reconstruction. We show, through both empirical and theoretical evidence, the potential for a dramatic reduction in computational time with little increase in human effort. We also discuss the intrinsic connection between EM-type algorithms and the Gibbs sampler, and the possibility of using the techniques presented here to speed up the latter. The main conclusion of the paper is that, with the help of statistical considerations, it is possible to construct algorithms that are simple, stable and fast.},
 author = {{Xiao-{L}i} {M}eng and {{D}avid} {van~{D}yk} },
 journal = {Journal of the {R}oyal {S}tatistical {S}ociety. {S}eries {B} ({M}ethodological)},
 number = {3},
 pages = {511--567},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {The {EM} {A}lgorithm - {A}n {O}ld {F}olk-{S}ong {S}ung to a {F}ast {N}ew {T}une},
 volume = {59},
 year = {1997}
}

@article{mclachlanAECM,
author = {Mclachlan, G. and Peel, D. and Bean, Richard},
year = {2003},
month = {01},
pages = {379-388},
title = {Modelling {H}igh-{D}imensional {D}ata by {M}ixtures of {F}actor {A}nalyzers},
volume = {41},
journal = {Computational {S}tatistics \& {D}ata {A}nalysis},
doi = {10.1016/S0167-9473(02)00183-4}
}

@article{DempsterEM,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the {R}oyal {S}tatistical {S}ociety. {S}eries {B} ({M}ethodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum {L}ikelihood from {I}ncomplete {D}ata via the {EM} {A}lgorithm},
 volume = {39},
 year = {1977}
}

@article{BIC,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2958889},
 abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
 author = {Gideon Schwarz},
 journal = {The Annals of Statistics},
 number = {2},
 pages = {461--464},
 publisher = {Institute of Mathematical Statistics},
 title = {Estimating the Dimension of a Model},
 volume = {6},
 year = {1978}
}

@techreport{ghahramani1996algorithm,
  title={The {EM} {A}lgorithm for {M}ixtures of {F}actor {A}nalyzers},
  author={Ghahramani, Zoubin and Hinton, Geoffrey E and others},
  year = {1997}
}

@article{kaya2015adaptive,
    title={Adaptive {M}ixtures of {F}actor {A}nalyzers},
    author={Kaya, Heysem and Salah, Albert Ali},
    journal={arXiv {P}reprint arXiv:1507.02801},
    year={2015}
}

@book{DimRed,
    author    = {P\'{a}draig Cunningham}, 
    title     = {Introduction to {L}earning {P}rinciples for {M}ultimedia {D}ata},
    publisher = {Springer},
    year      = 2008
}

@article{KerstenJens2014Sfsa,
    issn = {0031-3203},
    abstract = {<p>A new expectation maximization (EM) algorithm for time-critical supervised classification tasks in remote sensing is proposed. Compared to standard EM and other approaches, it has the following advantages: (1) No knowledge about the class distributions is needed. (2) The number of components is estimated. (3) It does not require careful initialization. (4) Singular estimates are avoided due to the ability of pruning components. (5) The best discriminating features are identified simultaneously. (6) The features are identified by incorporating Mahalanobis distances.</p> <p>Three experiments are carried out in order to demonstrate the relevance of the method. The main findings are the following: (1) Feature selection is very important in terms of prediction quality of models. (2) In the experiments the proposed method estimates better models than other state-of-the-art methods. (3) The incorporation of Mahalanobis distances is very valuable for the identification of relevant features. (4) The proposed method is more robust than the compared methods. (5) In case of complex data distributions the new approach is able to provide better results.</p>},
    journal = {{P}attern {R}ecognition},
    pages = {2582--2595},
    volume = {47},
    publisher = {Elsevier Ltd},
    number = {8},
    year = {2014},
    title = {Simultaneous {F}eature {S}election and {G}aussian {M}ixture {M}odel {E}stimation for {S}upervised {C}lassification {P}roblems},
    language = {eng},
    author = {Kersten, Jens},
    keywords = {Gaussian Mixture Models ; Clustering ; Feature Selection ; Feature Saliency ; Expectation Maximization ; Supervised Learning ; Remote Sensing ; Computer Science},
}

@inproceedings{ghahramani2000variational,
    title={Variational {I}nference for {B}ayesian {M}ixtures of {F}actor {A}nalysers},
    author={Ghahramani, Zoubin and Beal, Matthew J},
    booktitle={{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
    pages={449--455},
    year={2000}
}

@article{figueiredo2002unsupervised,
    title={{U}nsupervised {L}earning of {F}inite {M}ixture {M}odels},
    author={Figueiredo, Mario A. T. and Jain, Anil K.},
    journal={{IEEE} {T}ransactions on {P}attern {A}nalysis \& {M}achine {I}ntelligence},
    number={3},
    pages={381--396},
    year={2002},
    publisher={Ieee}
}


@article{YANG20123950,
    title = {A {R}obust {E}{M} {C}lustering {A}lgorithm for {G}aussian {M}ixture {M}odels},
    journal = "{P}attern {R}ecognition",
    volume = "45",
    number = "11",
    pages = "3950 - 3961",
    year = "2012",
    issn = "0031-3203",
    doi = "https://doi.org/10.1016/j.patcog.2012.04.031",
    url = "http://www.sciencedirect.com/science/article/pii/S0031320312002117",
    author = "Miin-Shen Yang and Chien-Yo Lai and Chih-Ying Lin",
    keywords = "Cluster analysis, EM algorithm, Gaussian mixture model, Robust EM, Initialization, Number of clusters",
    abstract = "Clustering is a useful tool for finding structure in a data set. The mixture likelihood approach to clustering is a popular clustering method, in which the EM algorithm is the most used method. However, the EM algorithm for Gaussian mixture models is quite sensitive to initial values and the number of its components needs to be given a priori. To resolve these drawbacks of the EM, we develop a robust EM clustering algorithm for Gaussian mixture models, first creating a new way to solve these initialization problems. We then construct a schema to automatically obtain an optimal number of clusters. Therefore, the proposed robust EM algorithm is robust to initialization and also different cluster volumes with automatically obtaining an optimal number of clusters. Some experimental examples are used to compare our robust EM algorithm with existing clustering methods. The results demonstrate the superiority and usefulness of our proposed method."
}

@article{DempsterA.P.1977MLfI,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
issn = {0035-9246},
journal = {Journal of the {R}oyal {S}tatistical {S}ociety: {S}eries {B} ({M}ethodological)},
keywords = {Maximum Likelihood ; Incomplete Data ; Em Algorithm ; Posterior Mode},
number = {1},
pages = {1-22},
title = {Maximum {L}ikelihood {F}rom {I}ncomplete {D}ata via the {E}{M} {A}lgorithm},
volume = {39},
year = {1977},
}

@article{MengXiao-Li1993MLEv,
abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler CM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
author = {Meng, Xiao-Li and Rubin, Donald B.},
issn = {00063444},
journal = {Biometrika},
keywords = {Mathematics -- Applied mathematics -- Statistics ; Mathematics -- Mathematical values -- Mathematical variables ; Information science -- Information management -- Data management ; Mathematics -- Mathematical expressions -- Mathematical functions ; Applied sciences -- Computer science -- Algorithms ; Mathematics -- Pure mathematics -- Probability theory ; Mathematics -- Applied mathematics -- Statistics ; Mathematics -- Applied mathematics -- Statistics ; Mathematics -- Mathematical values -- Critical values ; Mathematics -- Pure mathematics -- Linear algebra},
language = {eng},
number = {2},
pages = {267-278},
publisher = {Biometrika Trust},
title = {{M}aximum {L}ikelihood {E}stimation via the {E}{C}{M} {A}lgorithm: {A} {G}eneral {F}ramework},
volume = {80},
year = {1993},
}

@article{xmeans,
author = {Pelleg, Dan and Moore, Andrew},
year = {2002},
month = {01},
pages = {},
title = {X-means: {E}xtending {K}-means with {E}fficient {E}stimation of the {N}umber of {C}lusters},
journal = {Machine {L}earning, p}
}

@article{BleiDavidM2017VIAR,
abstract = {<p>One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale...},
author = {Blei, David M and Kucukelbir, Alp and Mc{A}uliffe, Jon D},
issn = {0162-1459},
journal = {Journal of the {A}merican {S}tatistical {A}ssociation},
keywords = {Algorithms ; Computationally Intensive Methods ; Statistical Computing ; Statistics},
language = {eng},
number = {518},
pages = {859-877},
publisher = {Taylor & Francis},
title = {Variational {I}nference: {A} {R}eview for {S}tatisticians},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1285773},
volume = {112},
year = {2017},
}

@inproceedings{SalahA.A2004Imof,
abstract = {<p>A mixture of factor analyzer is a semiparametric density estimator that performs clustering and dimensionality reduction in each cluster (component) simultaneously. It performs nonlinear dimensionality reduction by modeling the density as a mixture of local linear models. The approach can be used for classification by modeling each class-conditional density using a mixture model and the complete data is then a mixture of mixtures. We propose an incremental mixture of factor analysis algorithm where the number of components (local models) in the mixture and the number of factors in each component (local dimensionality) are determined adaptively. Our results on different pattern classification tasks prove the utility of our approach and indicate that our algorithms find a good trade-off between model complexity and accuracy.</p>},
author = {Salah, A.A and Alpaydin, E},
booktitle = {Proceedings of the 17th {I}nternational {C}onference on {P}attern {R}ecognition, 2004. {ICPR} 2004},
isbn = {0769521282},
issn = {10514651},
keywords = {Gaussian Processes ; Gaussian Noise ; Iterative Algorithms ; Performance Analysis ; Algorithm Design and Analysis ; Pattern Classification ; Testing ; Covariance Matrix ; Maximum Likelihood Estimation ; Pattern Matching ; Engineering ; Computer Science},
language = {eng},
pages = {276-279 Vol.1},
publisher = {IEEE},
title = {Incremental {M}ixtures of {F}actor {A}nalysers},
volume = {1},
year = {2004},
}

@article{doi:10.1111/1467-9868.00082,
author = {Meng, Xiao-Li and Van Dyk, David},
title = {The {E}{M} {A}lgorithm — {A}n {O}ld {F}olk-song {S}ung to a {F}ast {N}ew {T}une},
journal = {Journal of the {R}oyal {S}tatistical {S}ociety: {S}eries {B} ({S}tatistical {M}ethodology)},
volume = {59},
number = {3},
pages = {511-567},
keywords = {Data augmentation, Expectation–conditional maximization algorithm, Expectation–conditional maximization either algorithm, Gibbs sampler, Incomplete data, Markov chain Monte Carlo method, Missing data, Model reduction, Multivariate t-distributions, Poisson model, Positron emission tomogrpahy, Rate of convergence, Sage algorithm},
doi = {10.1111/1467-9868.00082},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00082},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00082},
abstract = {Celebrating the 20th anniversary of the presentation of the paper by Dempster, Laird and Rubin which popularized the EM algorithm, we investigate, after a brief historical account, strategies that aim to make the EM algorithm converge faster while maintaining its simplicity and stability (e.g. automatic monotone convergence in likelihood). First we introduce the idea of a ‘working parameter’ to facilitate the search for efficient data augmentation schemes and thus fast EM implementations. Second, summarizing various recent extensions of the EM algorithm, we formulate a general alternating expectation–conditional maximization algorithm AECM that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations. We illustrate these methods using multivariate t-models with known or unknown degrees of freedom and Poisson models for image reconstruction. We show, through both empirical and theoretical evidence, the potential for a dramatic reduction in computational time with little increase in human effort. We also discuss the intrinsic connection between EM-type algorithms and the Gibbs sampler, and the possibility of using the techniques presented here to speed up the latter. The main conclusion of the paper is that, with the help of statistical considerations, it is possible to construct algorithms that are simple, stable and fast.},
year = {1997}
}


@article{MclachlanG.J2003Mhdb,
abstract = {<p>We focus on mixtures of factor analyzers from the perspective of a method for model-based density estimation from high-dimensional data, and hence for the clustering of such data. This approach enables a normal mixture model to be fitted to a sample of n data points of dimension p, where p is large relative to n. The number of free parameters is controlled through the dimension of the latent factor space. By working in this reduced space, it allows a model for each component-covariance matrix with complexity lying between that of the isotropic and full covariance structure models. We shall illustrate the use of mixtures of factor analyzers in a practical example that considers the clustering of cell lines on the basis of gene expressions from microarray experiments. </p>},
issn = {0167-9473},
journal = {Computational {S}tatistics and {D}ata {A}nalysis},
keywords = {Mixture Modelling ; Factor Analyzers ; Em Algorithm ; Mixture Modelling ; Factor Analyzers ; Em Algorithm ; Mathematics},
language = {eng},
number = {3-4},
pages = {379-388},
author = {Mclachlan, G.J and Peel, D and Bean, R.W},
publisher = {Elsevier B.V},
title = {Modelling {H}igh-dimensional {D}ata by {M}ixtures of {F}actor {A}nalyzers},
volume = {41},
year = {2003},
}

@article{Jian-HuaZhao2008FMEf,
abstract = {<p>In this brief, we propose a fast expectation conditional maximization (ECM) algorithm for maximum-likelihood (ML) estimation of mixtures of factor analyzers (MFA). Unlike the existing expectation-maximization (EM) algorithms such as the EM in Ghahramani and Hinton, 1996, and the alternating ECM (AECM) in McLachlan and Peel, 2003, where the missing data contains component-indicator vectors as well as latent factors, the missing data in our ECM consists of component-indicator vectors only. The novelty of our algorithm is that closed-form expressions in all conditional maximization (CM) steps are obtained explicitly, instead of resorting to numerical optimization methods. As revealed by experiments, the convergence of our ECM is substantially faster than EM and AECM regardless of whether assessed by central processing unit (CPU) time or number of iterations.</p>},
issn = {1045-9227},
journal = {{IEEE} {T}ransactions on {N}eural {N}etworks},
keywords = {Maximum Likelihood Estimation ; Algorithm Design and Analysis ; Electrochemical Machining ; Convergence ; Statistics ; Closed-Form Solution ; Central Processing Unit ; Stability ; Covariance Matrix ; Optimization Methods ; Alternating Expectation Conditional Maximization (Aecm) ; Expectation Conditional Maximization (Ecm) ; Expectation Maximization (Em) ; Maximum-Likelihood Estimation (Mle) ; Mixture of Factor Analyzers (Mfa) ; Engineering ; Statistics ; Computer Science ; Anatomy & Physiology},
language = {eng},
number = {11},
pages = {1956-1961},
author = {Jian-Hua Zhao and Yu, P.L.H},
publisher = {IEEE},
title = {Fast {M}{L} {E}stimation for the {M}ixture of {F}actor {A}nalyzers via an {E}{C}{M} {A}lgorithm},
volume = {19},
year = {2008},
doi = {https://doi.org/10.1109/TNN.2008.2003467}
}


@article{MCLACHLAN20075327,
title = {Extension of the {M}ixture of {F}actor {A}nalyzers {M}odel to {I}ncorporate the {M}ultivariate t-distribution},
journal = "Computational {S}tatistics and {D}ata {A}nalysis",
volume = "51",
number = "11",
pages = "5327 - 5338",
year = "2007",
note = "Advances in Mixture Models",
issn = "0167-9473",
doi = "https://doi.org/10.1016/j.csda.2006.09.015",
url = "http://www.sciencedirect.com/science/article/pii/S0167947306003410",
author = "G.J. McLachlan and R.W. Bean and L. {Ben-Tovim Jones}",
keywords = "Mixture modelling, Factor analyzers, Multivariate -distribution, EM algorithm",
abstract = "Mixtures of factor analyzers enable model-based density estimation to be undertaken for high-dimensional data, where the number of observations n is small relative to their dimension p. However, this approach is sensitive to outliers as it is based on a mixture model in which the multivariate normal family of distributions is assumed for the component error and factor distributions. An extension to mixtures of t-factor analyzers is considered, whereby the multivariate t-family is adopted for the component error and factor distributions. An EM-based algorithm is developed for the fitting of mixtures of t-factor analyzers. Its application is demonstrated in the clustering of some microarray gene-expression data."
}


@article{WangWan-Lun2020Alom,
issn = {1133-0686},
journal = {{TEST}},
language = {eng},
author = {Wang, Wan-Lun and Lin, Tsung-I},
title = {Automated {L}earning of {M}ixtures of {F}actor {A}nalysis {M}odels with {M}issing {I}nformation},
year = {2020},
}


@article{ZhaoJianhua2014Alof,
abstract = {<p>In the application of the popular maximum likelihood method to factor analysis, the number of factors is commonly determined through a two-stage procedure, in which stage 1 performs parameter estimation for a set of candidate models and then stage 2 chooses the best according to certain model selection criterion. Usually, to obtain satisfactory performance, a large set of candidates is used and this procedure suffers a heavy computational burden. To overcome this problem, a novel one-stage algorithm is proposed in which parameter estimation and model selection are integrated in a single algorithm. This is obtained by maximizing the criterion with respect to model parameters and the number of factors jointly, rather than separately. The proposed algorithm is then extended to accommodate incomplete data. Experiments on a number of complete/incomplete synthetic and real data reveal that the proposed algorithm is as effective as the existing two-stage procedure while being much more...},
issn = {0167-9473},
journal = {Computational {S}tatistics and {D}ata {A}nalysis},
keywords = {Factor Analysis ; Model Selection ; Maximum Likelihood ; Incomplete Data ; Factor Analysis ; Model Selection ; Maximum Likelihood ; Incomplete Data ; CM ; Em ; Mathematics},
language = {eng},
pages = {205-218},
author = {Zhao, Jianhua and Shi, Lei},
publisher = {Elsevier B.V},
title = {Automated {L}earning of {F}actor {A}nalysis with {C}omplete and {I}ncomplete {D}ata},
volume = {72},
year = {2014},
}
@article{amfa,
author = {Wang, Wan-Lun and Lin, Tsung},
year = {2020},
month = {01},
pages = {},
title = {Automated {L}earning of {M}ixtures of {F}actor {A}nalysis {M}odels with {M}issing {I}nformation},
volume = {29},
journal = {{TEST}},
doi = {10.1007/s11749-020-00702-6}
}

@article{Ledermann37,

    abstract = {Since the factor problem is reduced mathematically to the expression of the obtained correlational matrix in terms of a matrix of lower rank, criteria for the determination of this lower rank are of first importance. These criteria are investigated by means of certain mathematical deductions, and brought into relation with Spearman's and Thurstone's factor theories.},

    journal = {Psychometrika},

    language = {eng},

    pages = {85-93},

    author = {Ledermann, Walter},

    title = {On the {R}ank of the {R}educed {C}orrelational {M}atrix in {M}ultiple-factor {A}nalysis},

    volume = {2},

    year = {1937},

	doi = {https://doi.org/10.1007/BF02288062},

}

@article{varimax,
    
    abstract = {An analytic criterion for rotation is defined. The scientific advantage of analytic criteria over subjective (graphical) rotational procedures is discussed. Carroll's criterion and the quartimax criterion are briefly reviewed; the varimax criterion is outlined in detail and contrasted both logically and numerically with the quartimax criterion. It is shown that thenormal varimax solution probably coincides closely to the application of the principle of simple structure. However, it is proposed that the ultimate criterion of a rotational procedure is factorial invariance, not simple structure—although the two notions appear to be highly related. The normal varimax criterion is shown to be a two-dimensional generalization of the classic Spearman case, i.e., it shows perfect factorial invariance for two pure clusters. An example is given of the invariance of a normal varimax solution for more than two factors. The oblique normal varimax criterion is stated. A computational outline for the orthogonal normal varimax is appended.},
    
    journal = {Psychometrika},
    
    language = {eng},
    
    pages = {187-200},
    
    author = {Kaiser, Henry F.},
    
    title = {The {V}arimax {C}riterion for {A}nalytic {R}otation in {F}actor {A}nalysis},
    
    volume = {23},
    
    year = {1958},
    doi = {https://doi.org/10.1007/BF02289233},
}

@phdthesis{bealthesis,
  author       = {Matthew J Beal}, 
  title        = {Variational {A}lgorithms for {A}pproximate {B}ayesian {I}nference},
  school       = {University College London},
  year         = 2003,
  address      = {The Gatsby Computational Neuroscience Unit, University College London, 17 Queen Square, London WC1N 3AR},
  month        = 5,
}
@Manual{IMIFA,
    author = {Keefe Murphy and Cinzia Viroli and Isobel Claire Gormley},
    title = {\pkg{IMIFA}: {I}nfinite {M}ixtures of {I}nfinite {F}actor {A}nalysers and {R}elated {M}odels},
    year = {2021},
    note = {\textsf{R} package version 2.1.8},
    url = {https://cran.r-project.org/package=IMIFA},
  }

@Manual{autoMFA,
    author = {John Davey},
    title = {\pkg{autoMFA}: {A}lgorithms for {A}utomatically {F}itting {M}{F}{A} {M}odels},
    year = {2021},
    note = {\textsf{R} package version 1.0.0},
    url = {https://cran.r-project.org/package=autoMFA},
  }

@Article{fabMix,
    title = {Overfitting {B}ayesian {M}ixtures of {F}actor {A}nalyzers {w}ith {a}n {U}nknown {N}umber of {C}omponents.},
    author = {Panagiotis Papastamoulis},
    journal = {Computational Statistics and Data Analysis},
    year = {2018},
    pages = {220--234},
    volume = {124},
    doi = {10.1016/j.csda.2018.03.007},
  }

@Manual{R,
    title = {\proglang{R}: {A} {L}anguage and {E}nvironment for {S}tatistical {C}omputing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
@Manual{EMMIXmfa,
    title = {\pkg{EMMIXmfa}: {M}ixture {M}odels with {C}omponent-{W}ise {F}actor {A}nalyzers},
    author = {Suren Rathnayake and Geoffrey McLachlan and David Peel and Jangsun Baek and {R Core Team}},
    year = {2019},
    note = {R package version 2.0.11},
    url = {https://CRAN.R-project.org/package=EMMIXmfa},
  }

