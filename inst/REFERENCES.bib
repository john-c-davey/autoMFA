@techreport{ghahramani1996algorithm,
  title={The {EM} {A}lgorithm for {M}ixtures of {F}actor {A}nalyzers},
  institution = The {U}niversity of {T}oronto,
  author={Ghahramani, Zoubin and Hinton, Geoffrey E and others},
  year = {1997}
}

@article{kaya2015adaptive,
	title={Adaptive {M}ixtures of {F}actor {A}nalyzers},
	author={Kaya, Heysem and Salah, Albert Ali},
	journal={arXiv preprint arXiv:1507.02801},
	year={2015}
}

@inproceedings{ghahramani2000variational,
	title={Variational inference for {B}ayesian {M}ixtures of {F}actor {A}nalysers},
	author={Ghahramani, Zoubin and Beal, Matthew J},
	booktitle={Advances in neural information processing systems},
	pages={449--455},
	year={2000}
}

@inproceedings{SalahA.A2004Imof,
abstract = {<p>A mixture of factor analyzer is a semiparametric density estimator that performs clustering and dimensionality reduction in each cluster (component) simultaneously. It performs nonlinear dimensionality reduction by modeling the density as a mixture of local linear models. The approach can be used for classification by modeling each class-conditional density using a mixture model and the complete data is then a mixture of mixtures. We propose an incremental mixture of factor analysis algorithm where the number of components (local models) in the mixture and the number of factors in each component (local dimensionality) are determined adaptively. Our results on different pattern classification tasks prove the utility of our approach and indicate that our algorithms find a good trade-off between model complexity and accuracy.</p>},
author = {Salah, A.A and Alpaydin, E},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004},
isbn = {0769521282},
issn = {10514651},
keywords = {Gaussian Processes ; Gaussian Noise ; Iterative Algorithms ; Performance Analysis ; Algorithm Design and Analysis ; Pattern Classification ; Testing ; Covariance Matrix ; Maximum Likelihood Estimation ; Pattern Matching ; Engineering ; Computer Science},
language = {eng},
pages = {276-279 Vol.1},
publisher = {IEEE},
title = {Incremental mixtures of factor analysers},
volume = {1},
year = {2004},
}

@article{Jian-HuaZhao2008FMEf,
abstract = {<p>In this brief, we propose a fast expectation conditional maximization (ECM) algorithm for maximum-likelihood (ML) estimation of mixtures of factor analyzers (MFA). Unlike the existing expectation-maximization (EM) algorithms such as the EM in Ghahramani and Hinton, 1996, and the alternating ECM (AECM) in McLachlan and Peel, 2003, where the missing data contains component-indicator vectors as well as latent factors, the missing data in our ECM consists of component-indicator vectors only. The novelty of our algorithm is that closed-form expressions in all conditional maximization (CM) steps are obtained explicitly, instead of resorting to numerical optimization methods. As revealed by experiments, the convergence of our ECM is substantially faster than EM and AECM regardless of whether assessed by central processing unit (CPU) time or number of iterations.</p>},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Maximum Likelihood Estimation ; Algorithm Design and Analysis ; Electrochemical Machining ; Convergence ; Statistics ; Closed-Form Solution ; Central Processing Unit ; Stability ; Covariance Matrix ; Optimization Methods ; Alternating Expectation Conditional Maximization (Aecm) ; Expectation Conditional Maximization (Ecm) ; Expectation Maximization (Em) ; Maximum-Likelihood Estimation (Mle) ; Mixture of Factor Analyzers (Mfa) ; Engineering ; Statistics ; Computer Science ; Anatomy & Physiology},
language = {eng},
number = {11},
pages = {1956-1961},
author = {Jian-Hua Zhao and Yu, P.L.H},
publisher = {IEEE},
title = {Fast {ML} Estimation for the {M}ixture of {F}actor {A}nalyzers via an {ECM} {A}lgorithm},
volume = {19},
year = {2008},
}


@article{WangWan-Lun2020Alom,
issn = {1133-0686},
journal = {TEST},
language = {eng},
author = {Wang, Wan-Lun and Lin, Tsung-I},
title = {Automated learning of mixtures of factor analysis models with missing information},
year = {2020},
}


@article{ZhaoJianhua2014Alof,
abstract = {<p>In the application of the popular maximum likelihood method to factor analysis, the number of factors is commonly determined through a two-stage procedure, in which stage 1 performs parameter estimation for a set of candidate models and then stage 2 chooses the best according to certain model selection criterion. Usually, to obtain satisfactory performance, a large set of candidates is used and this procedure suffers a heavy computational burden. To overcome this problem, a novel one-stage algorithm is proposed in which parameter estimation and model selection are integrated in a single algorithm. This is obtained by maximizing the criterion with respect to model parameters and the number of factors jointly, rather than separately. The proposed algorithm is then extended to accommodate incomplete data. Experiments on a number of complete/incomplete synthetic and real data reveal that the proposed algorithm is as effective as the existing two-stage procedure while being much more...},
issn = {0167-9473},
journal = {Computational Statistics and Data Analysis},
keywords = {Factor Analysis ; Model Selection ; Maximum Likelihood ; Incomplete Data ; Factor Analysis ; Model Selection ; Maximum Likelihood ; Incomplete Data ; CM ; Em ; Mathematics},
language = {eng},
pages = {205-218},
author = {Zhao, Jianhua and Shi, Lei},
publisher = {Elsevier B.V},
title = {Automated learning of factor analysis with complete and incomplete data},
volume = {72},
year = {2014},
}









